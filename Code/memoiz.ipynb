{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisi FLOPs per ResNet-18 e caching\n",
    "Questo notebook calcola i FLOPs dei primi layer di ResNet-18 e analizza scenari di caching per ridurre il carico computazionale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione dei layer convoluzionali nei primi 3 blocchi della ResNet-18 come da documento\n",
    "\n",
    "layers = [\n",
    "    (\"Conv1\", 112, 112, 7, 3, 64, 1),\n",
    "    (\"Block1_Conv1\", 56, 56, 3, 64, 64, 1),\n",
    "    (\"Block1_Conv2\", 56, 56, 3, 64, 64, 1),\n",
    "    (\"Block2_Conv1\", 56, 56, 3, 64, 64, 1),\n",
    "    (\"Block2_Conv2\", 56, 56, 3, 64, 64, 1),\n",
    "    (\"Block3_Conv1\", 28, 28, 3, 64, 128, 1),\n",
    "    (\"Block3_Conv2\", 28, 28, 3, 128, 128, 1),\n",
    "    (\"Block4_Conv1\", 28, 28, 3, 128, 128, 1),\n",
    "    (\"Block4_Conv2\", 28, 28, 3, 128, 128, 1),\n",
    "    (\"Block5_Conv1\", 14, 14, 3, 128, 256, 1),\n",
    "    (\"Block5_Conv2\", 14, 14, 3, 256, 256, 1),\n",
    "    (\"Block6_Conv1\", 14, 14, 3, 256, 256, 1),\n",
    "    (\"Block6_Conv2\", 14, 14, 3, 256, 256, 1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1: 0.2360 GFLOPs\n",
      "Block1_Conv1: 0.2312 GFLOPs\n",
      "Block1_Conv2: 0.2312 GFLOPs\n",
      "Block2_Conv1: 0.2312 GFLOPs\n",
      "Block2_Conv2: 0.2312 GFLOPs\n",
      "Block3_Conv1: 0.1156 GFLOPs\n",
      "Block3_Conv2: 0.2312 GFLOPs\n",
      "Block4_Conv1: 0.2312 GFLOPs\n",
      "Block4_Conv2: 0.2312 GFLOPs\n",
      "Block5_Conv1: 0.1156 GFLOPs\n",
      "Block5_Conv2: 0.2312 GFLOPs\n",
      "Block6_Conv1: 0.2312 GFLOPs\n",
      "Block6_Conv2: 0.2312 GFLOPs\n",
      "Total GFLOPs: 2.7793 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "# Calcolo FLOPs per ciascuna convoluzione\n",
    "results = []\n",
    "for name, H, W, K, C_in, C_out, n_convs in layers:\n",
    "    flops = 2 * H * W * (K ** 2) * C_in * C_out * n_convs\n",
    "    results.append((name, flops))\n",
    "\n",
    "for name, flops in results:\n",
    "    print(f\"{name}: {flops / 1e9:.4f} GFLOPs\")\n",
    "\n",
    "total_flops = sum(flops for _, flops in results)\n",
    "print(f\"Total GFLOPs: {total_flops / 1e9:.4f} GFLOPs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039841c2",
   "metadata": {},
   "source": [
    "### Optimistic Scenario:\n",
    "In this scenario we assume that's ideally possible to perform a truncation during the convolutional operation, expecially we cna assume that we can truncate the output channel to avoid the computation of the 20% of the channels that we cached. (No way to do it using actual libraries and i think hard to implement ... does it make sense to do it?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " --- Caching Analysis in optimistic scenario ---\n",
      "Total FLOPs before caching: 2.7793 GFLOPs\n",
      "FLOPs of cached layers: 2.2235 GFLOPs (-20.00% of total FLOPs)\n",
      "Total FLOPs after caching: 3.6132 GFLOPs, which is 130.00% of the total FLOPs\n",
      "|-> Considering sum of \t the caching optimization 2.2235 and \n",
      " \t\t\t the student model cost 1.3897\n",
      "FLOPs saved: -0.8338 GFLOPs, which is -30.00% of the total FLOPs\n"
     ]
    }
   ],
   "source": [
    "# Scenario ottimistico con caching del 20%\n",
    "fraction_cached = 0.2\n",
    "fraction_uncached = 1.0 - fraction_cached\n",
    "student_model_cost = 0.5 * total_flops\n",
    "flops_after_caching = fraction_uncached * total_flops + student_model_cost\n",
    "flops_saved = total_flops - flops_after_caching\n",
    "\n",
    "print(\"\\n --- Caching Analysis in optimistic scenario ---\")\n",
    "print(f\"Total FLOPs before caching: {float(total_flops) / 1e9:.4f} GFLOPs\")\n",
    "print(f\"FLOPs of cached layers: {float(fraction_uncached * total_flops) / 1e9:.4f} GFLOPs ({-fraction_cached * 100:.2f}% of total FLOPs)\")\n",
    "print(f\"Total FLOPs after caching: {float(flops_after_caching) / 1e9:.4f} GFLOPs, which is {float(flops_after_caching) / float(total_flops) * 100:.2f}% of the total FLOPs\")\n",
    "print(f\"|-> Considering sum of \\t the caching optimization {float(fraction_uncached * total_flops) / 1e9:.4f} and \\n \\t\\t\\t the student model cost {float(student_model_cost) / 1e9:.4f}\")\n",
    "print(f\"FLOPs saved: {float(flops_saved) / 1e9:.4f} GFLOPs, which is {float(flops_saved) / float(total_flops) * 100:.2f}% of the total FLOPs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb18d548",
   "metadata": {},
   "source": [
    "### Realistic Scenario:\n",
    "In this scenario we assume that we can cache the output of the first layers and reuse it for the next, but we still need to compute the full output of the first layer. This is a more realistic approach as it reflects current capabilities in deep learning frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FLOPs before caching: 2.7793 GFLOPs\n",
      "FLOPs of cached layers: 2.7793 GFLOPs (no caching)\n",
      "Total FLOPs after caching: 4.1690 GFLOPs, which is 150.00% of the total FLOPs\n",
      "|-> Considering sum of \t the caching optimization 2.7793 and \n",
      " \t\t\t the student model cost 1.3897\n",
      "FLOPs saved: -1.3897 GFLOPs, which is -50.00% of the total FLOPs\n"
     ]
    }
   ],
   "source": [
    "# Scenario pessimista: nessun risparmio da caching\n",
    "flops_after_caching_pessimistic = total_flops + student_model_cost\n",
    "flops_saved_pessimistic = total_flops - flops_after_caching_pessimistic\n",
    "\n",
    "print(f\"Total FLOPs before caching: {float(total_flops) / 1e9:.4f} GFLOPs\")\n",
    "print(f\"FLOPs of cached layers: {total_flops / 1e9:.4f} GFLOPs (no caching)\")\n",
    "print(f\"Total FLOPs after caching: {flops_after_caching_pessimistic / 1e9:.4f} GFLOPs, which is {flops_after_caching_pessimistic / total_flops * 100:.2f}% of the total FLOPs\")\n",
    "print(f\"|-> Considering sum of \\t the caching optimization {float(total_flops) / 1e9:.4f} and \\n \\t\\t\\t the student model cost {float(student_model_cost) / 1e9:.4f}\")\n",
    "print(f\"FLOPs saved: {flops_saved_pessimistic / 1e9:.4f} GFLOPs, which is {flops_saved_pessimistic / total_flops * 100:.2f}% of the total FLOPs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db755e",
   "metadata": {},
   "source": [
    "### Optimistic Scenario with Caching:\n",
    "In this scenario, we assume that we can reuse all the cached outputs from the first layers for the following, effectively reducing the computational load significantly. The document discuss show that this approach leads to a significant reduction in accuracy and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FLOPs before caching: 2.7793 GFLOPs\n",
      "FLOPs of cached layers: 0.0000 GFLOPs (-100.00% of total FLOPs)\n",
      "Total FLOPs after caching: 1.3897 GFLOPs, which is 50.00% of the total FLOPs\n",
      "|-> Considering sum of \t the caching optimization 0.0000 and \n",
      " \t\t\t the student model cost 1.3897\n",
      "FLOPs saved: 1.3897 GFLOPs, which is 50.00% of the total FLOPs\n"
     ]
    }
   ],
   "source": [
    "# Scenario ottimistico con caching completo\n",
    "fraction_cached_optimistic = 1.0\n",
    "fraction_uncached_optimistic = 0.0\n",
    "flops_after_caching_optimistic = fraction_uncached_optimistic * total_flops + student_model_cost\n",
    "flops_saved_optimistic = total_flops - flops_after_caching_optimistic\n",
    "\n",
    "print(f\"Total FLOPs before caching: {float(total_flops) / 1e9:.4f} GFLOPs\")\n",
    "print(f\"FLOPs of cached layers: {float(fraction_uncached_optimistic * total_flops) / 1e9:.4f} GFLOPs ({-fraction_cached_optimistic * 100:.2f}% of total FLOPs)\")\n",
    "print(f\"Total FLOPs after caching: {float(flops_after_caching_optimistic) / 1e9:.4f} GFLOPs, which is {float(flops_after_caching_optimistic) / float(total_flops) * 100:.2f}% of the total FLOPs\")\n",
    "print(f\"|-> Considering sum of \\t the caching optimization {float(fraction_uncached_optimistic * total_flops) / 1e9:.4f} and \\n \\t\\t\\t the student model cost {float(student_model_cost) / 1e9:.4f}\")\n",
    "print(f\"FLOPs saved: {float(flops_saved_optimistic) / 1e9:.4f} GFLOPs, which is {float(flops_saved_optimistic) / float(total_flops) * 100:.2f}% of the total FLOPs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
