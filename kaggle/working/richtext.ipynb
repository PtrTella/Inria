{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T22:48:23.790345Z",
     "iopub.status.busy": "2025-07-16T22:48:23.790173Z",
     "iopub.status.idle": "2025-07-16T22:49:06.279479Z",
     "shell.execute_reply": "2025-07-16T22:49:06.278541Z",
     "shell.execute_reply.started": "2025-07-16T22:48:23.790329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from numpy import shape\n",
    "import random\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "from collections import OrderedDict, defaultdict\n",
    "from transformers import CLIPTokenizer, CLIPModel, CLIPTextModel, FlaxCLIPTextModel\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from tqdm import trange\n",
    "import threading, queue, math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T22:49:06.281537Z",
     "iopub.status.busy": "2025-07-16T22:49:06.280965Z",
     "iopub.status.idle": "2025-07-16T22:49:06.285765Z",
     "shell.execute_reply": "2025-07-16T22:49:06.285109Z",
     "shell.execute_reply.started": "2025-07-16T22:49:06.281518Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Se torch_xla è installato (TPU), importalo; altrimenti lo ignori\n",
    "try:\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    TPU_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TPU_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T22:49:06.287161Z",
     "iopub.status.busy": "2025-07-16T22:49:06.286871Z",
     "iopub.status.idle": "2025-07-16T22:49:06.528941Z",
     "shell.execute_reply": "2025-07-16T22:49:06.528111Z",
     "shell.execute_reply.started": "2025-07-16T22:49:06.287137Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PromptDatasetManager:\n",
    "    def __init__(self, repo_id=\"poloclub/diffusiondb\", filename=\"metadata.parquet\", max_size=None):\n",
    "        print(\"Scaricamento metadata.parquet...\")\n",
    "        self.meta_path = hf_hub_download(\n",
    "            repo_id=repo_id, repo_type=\"dataset\", filename=filename\n",
    "        )\n",
    "        if max_size:\n",
    "            self.df = pd.read_parquet(self.meta_path).head(max_size)\n",
    "        else:\n",
    "            self.df = pd.read_parquet(self.meta_path)\n",
    "        print(f\"Colonne disponibili: {list(self.df.columns)}\\n\")\n",
    "        print(f\"Dimensioni del DataFrame: {shape(self.df)}\")\n",
    "    \n",
    "    def get_path(self):\n",
    "        \"\"\"\n",
    "        Restituisce il percorso del file metadata.parquet scaricato.\n",
    "        \"\"\"\n",
    "        return self.meta_path\n",
    "\n",
    "    def _get_user_sessions(self,\n",
    "                          user_name: str,\n",
    "                          session_gap: int = 30) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Estrae dal DataFrame tutte le righe di un dato `user_name`, ordina per timestamp\n",
    "        e assegna un `session_id` incrementale ogni volta che l'intervallo tra richieste\n",
    "        successive supera `session_gap` minuti.\n",
    "        Deve restituire tutte le sessioni per un utente specifico.\n",
    "        \"\"\"\n",
    "        if 'user_name' not in self.df.columns or 'timestamp' not in self.df.columns:\n",
    "            raise ValueError(\"Le colonne 'user_name' e/o 'timestamp' non sono presenti.\")\n",
    "        df_user = self.df[self.df['user_name'] == user_name].copy()\n",
    "        if df_user.empty:\n",
    "            raise ValueError(f\"Nessun dato per l'utente {user_name}\")\n",
    "        df_user = df_user.sort_values('timestamp').reset_index(drop=True)\n",
    "        # Calcola delta in minuti\n",
    "        df_user['session_delta'] = df_user['timestamp'].diff().dt.total_seconds() / 60.0\n",
    "        # Assegna session_id\n",
    "        session_ids = []\n",
    "        current_id = 0\n",
    "        for delta in df_user['session_delta']:\n",
    "            if pd.isna(delta) or delta > session_gap:\n",
    "                current_id += 1\n",
    "            session_ids.append(current_id)\n",
    "        df_user['session_id'] = session_ids\n",
    "        return df_user\n",
    "\n",
    "        \n",
    "    def add_clip_embeddings_auto(\n",
    "        self,\n",
    "        output_path: str,\n",
    "        batch_size: int = 4096,\n",
    "        prefetch: int = 2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Streaming Parquet → batch → tokenizza su CPU → inferisce su\n",
    "        TPU (torch_xla) / GPU (CUDA) / CPU → scrive parquet con colonna `clip_emb`.\n",
    "        \"\"\"\n",
    "        reader = pq.ParquetFile(self.meta_path)\n",
    "        total_rows    = reader.metadata.num_rows\n",
    "\n",
    "        # --- Device selection e conteggio ---\n",
    "        if TPU_AVAILABLE:\n",
    "            device    = xm.xla_device()\n",
    "            n_devices = xm.xrt_world_size()\n",
    "            print(f\"▶ Using TPU: {n_devices} core(s), device={device}\")\n",
    "        elif torch.cuda.is_available():\n",
    "            n_devices = torch.cuda.device_count()\n",
    "            device    = torch.device(\"cuda\")\n",
    "            names     = [torch.cuda.get_device_name(i) for i in range(n_devices)]\n",
    "            print(f\"▶ Using {n_devices} GPU(s): {names}\")\n",
    "        else:\n",
    "            n_devices = 1\n",
    "            device    = torch.device(\"cpu\")\n",
    "            print(\"▶ CUDA/TPU non disponibile, uso CPU\")\n",
    "\n",
    "        # Adatto batch_size a multiplo di n_devices\n",
    "        per_dev_bs     = max(1, batch_size // n_devices)\n",
    "        batch_size_glb = per_dev_bs * n_devices\n",
    "        if batch_size_glb != batch_size:\n",
    "            print(f\"Aggiusto batch_size: {batch_size} → {batch_size_glb}  ({per_dev_bs}×{n_devices})\")\n",
    "        batch_size    = batch_size_glb\n",
    "        total_batches = math.ceil(total_rows / batch_size)\n",
    "\n",
    "        # --- Writer setup ---\n",
    "        schema_out = reader.schema_arrow.append(\n",
    "            pa.field(\"clip_emb\", pa.list_(pa.float32()))\n",
    "        )\n",
    "        writer = pq.ParquetWriter(output_path, schema_out, compression=None)\n",
    "\n",
    "        # --- Tokenizer & Model setup ---\n",
    "        tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\") \\\n",
    "                             .to(device).half()\n",
    "        if not TPU_AVAILABLE and n_devices > 1 and device.type == \"cuda\":\n",
    "            model = torch.nn.DataParallel(model)\n",
    "        model = torch.compile(model)\n",
    "        model.eval()\n",
    "\n",
    "        # Coda per pipeline producer/consumer\n",
    "        q = queue.Queue(prefetch)\n",
    "\n",
    "        def producer():\n",
    "            for batch in reader.iter_batches(batch_size=batch_size):\n",
    "                tbl     = pa.Table.from_batches([batch])\n",
    "                prompts = tbl.column(\"prompt\").to_pylist()\n",
    "                toks    = tokenizer(prompts,\n",
    "                                    padding=True,\n",
    "                                    truncation=True,\n",
    "                                    return_tensors=\"pt\")\n",
    "                q.put((tbl, toks))\n",
    "            q.put(None)\n",
    "\n",
    "        def consumer():\n",
    "            pbar = tqdm(total=total_batches, desc=\"CLIP embeddings\", unit=\"batch\")\n",
    "            while True:\n",
    "                item = q.get()\n",
    "                if item is None:\n",
    "                    break\n",
    "                tbl, toks = item\n",
    "                toks = {k: v.to(device, non_blocking=True) for k, v in toks.items()}\n",
    "\n",
    "                if TPU_AVAILABLE:\n",
    "                    with torch.no_grad():\n",
    "                        emb = model(**toks).pooler_output.cpu().numpy()\n",
    "                else:\n",
    "                    # mixed‑precision solo su CUDA\n",
    "                    with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")), torch.no_grad():\n",
    "                        emb = model(**toks).pooler_output.cpu().numpy()\n",
    "\n",
    "                tbl = tbl.append_column(\n",
    "                    \"clip_emb\",\n",
    "                    pa.array(emb.tolist(), type=pa.list_(pa.float32()))\n",
    "                )\n",
    "                writer.write_table(tbl)\n",
    "                pbar.update(1)\n",
    "            pbar.close()\n",
    "            writer.close()\n",
    "\n",
    "        t1 = threading.Thread(target=producer, daemon=True)\n",
    "        t2 = threading.Thread(target=consumer, daemon=True)\n",
    "        t1.start(); t2.start()\n",
    "        t1.join(); t2.join()\n",
    "\n",
    "        # Ricarico in pandas per usi successivi\n",
    "        self.df = pd.read_parquet(output_path)\n",
    "        print(f\"Embeddings calcolati e salvati in: {output_path}\")\n",
    "    \n",
    "    \n",
    "    def executeFunctionOnDataFrame(self, func, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Esegue una funzione `func` sul DataFrame e restituisce il risultato.\n",
    "        La funzione deve accettare un DataFrame come primo argomento.\n",
    "        \"\"\"\n",
    "        if not callable(func):\n",
    "            raise ValueError(\"Il parametro 'func' deve essere una funzione chiamabile.\")\n",
    "        return func(self.df, *args, **kwargs)\n",
    "\n",
    "    def  getDataFrame(self):\n",
    "        \"\"\"\n",
    "        Restituisce il DataFrame completo.\n",
    "        Se il DataFrame è molto grande, considera di usare `head()` per limitare le righe.\n",
    "        \"\"\"\n",
    "        return self.df\n",
    "    \n",
    "    def getPrompts(self, limit: int = None, shuffle: bool = True):\n",
    "        \"\"\"\n",
    "        Restituisce una lista di prompt dal DataFrame.\n",
    "        Se `limit` è specificato, restituisce solo i primi `limit` prompt.\n",
    "        \"\"\"\n",
    "        prompts = self.df['prompt'].dropna().tolist()\n",
    "        if shuffle:\n",
    "            random.shuffle(prompts)\n",
    "        if limit is not None:\n",
    "            prompts = prompts[:limit]\n",
    "        return prompts\n",
    "    \n",
    "    def getUsersPrompts(self, limit: int = None, shuffle: bool = True):\n",
    "        \"\"\"\n",
    "        Restituisce un dizionario di prompt per ogni utente.\n",
    "        Ogni chiave è il nome dell'utente, e il valore è una lista di prompt.\n",
    "        Se `limit` è specificato, restituisce solo i primi `limit` utenti.\n",
    "        \"\"\"\n",
    "        user_prompts = {}\n",
    "        users = self.df['user_name'].dropna().unique()\n",
    "        if shuffle:\n",
    "            random.shuffle(users)\n",
    "        if limit is not None:\n",
    "            users = users[:limit]\n",
    "        for user in users:\n",
    "            user_prompts[user] = self.df[self.df['user_name'] == user]['prompt'].dropna().tolist()\n",
    "        return user_prompts\n",
    "    \n",
    "    def getSessionsPrompts(self, session_gap: int = 30, limit: int = None, shuffle: bool = True):\n",
    "        \"\"\"\n",
    "        Restituisce un dizionario di sessioni per ogni utente.\n",
    "        Ogni chiave è il nome dell'utente, e il valore è un dizionario con `session_id` come chiave\n",
    "        e una lista di prompt come valore.\n",
    "        Se `limit` è specificato, restituisce solo i primi `limit` utenti.\n",
    "        \"\"\"\n",
    "        user_sessions = {}\n",
    "        users = self.df['user_name'].dropna().unique()\n",
    "        if shuffle:\n",
    "            random.shuffle(users)\n",
    "        if limit is not None:\n",
    "            users = users[:limit]\n",
    "        for user in users:\n",
    "            df_user = self._get_user_sessions(user, session_gap=session_gap)\n",
    "            if df_user.empty:\n",
    "                continue\n",
    "            sessions = defaultdict(list)\n",
    "            for _, row in df_user.iterrows():\n",
    "                sessions[row['session_id']].append(row['prompt'])\n",
    "            user_sessions[user] = dict(sessions)\n",
    "        return user_sessions\n",
    "    \n",
    "    def retRandomPrompt(self):\n",
    "        \"\"\"\n",
    "        Restituisce una funzione lambda che, ad ogni chiamata,\n",
    "        restituisce un nuovo prompt casuale dal database.\n",
    "        \"\"\"\n",
    "        prompts = self.df['prompt'].dropna().tolist()\n",
    "        return lambda: random.choice(prompts)\n",
    "    \n",
    "    def retRandomSession(self, session_gap: int = 30, max_prompts: int = None):\n",
    "        \"\"\"\n",
    "        Restituisce una funzione che genera sessioni di un utente casuale.\n",
    "        Ogni sessione ritorna i prompt di un utente casuale, se specificato,\n",
    "        i prompt per ogni sessione sono limitati a `max_prompts.\n",
    "        \"\"\"\n",
    "        users = self.df['user_name'].dropna().unique()\n",
    "        if len(users) == 0:\n",
    "            raise ValueError(\"Nessun utente trovato nel DataFrame.\")\n",
    "        \n",
    "        def generate_session():\n",
    "            user = random.choice(users)\n",
    "            df_user = self._get_user_sessions(user, session_gap)\n",
    "            if df_user.empty:\n",
    "                raise ValueError(f\"Nessuna sessione trovata per l'utente {user}\")\n",
    "            # Ritorno tutti i prompt della sessione se `max_prompts` non è specificato\n",
    "            if max_prompts is None:\n",
    "                return user, df_user['prompt'].dropna().tolist()\n",
    "            # Altrimenti, ritorno tutti i prompt della sessione\n",
    "            return \n",
    "\n",
    "        \n",
    "        return generate_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T22:49:06.530165Z",
     "iopub.status.busy": "2025-07-16T22:49:06.529884Z",
     "iopub.status.idle": "2025-07-16T23:05:20.527529Z",
     "shell.execute_reply": "2025-07-16T23:05:20.526764Z",
     "shell.execute_reply.started": "2025-07-16T22:49:06.530137Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "manager = PromptDatasetManager(max_size=10)\n",
    "manager.add_clip_embeddings_auto(\"dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T23:05:20.529592Z",
     "iopub.status.busy": "2025-07-16T23:05:20.52937Z",
     "iopub.status.idle": "2025-07-16T23:05:21.865418Z",
     "shell.execute_reply": "2025-07-16T23:05:21.864806Z",
     "shell.execute_reply.started": "2025-07-16T23:05:20.529574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "print(\"Esempio di utilizzo del PromptDatasetManager:\")\n",
    "print(\"Numero di prompt:\", len(manager.getPrompts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
