{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from numpy import shape\nimport random\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport torch\nfrom huggingface_hub import hf_hub_download\nfrom collections import OrderedDict, defaultdict\nfrom transformers import CLIPTokenizer, CLIPModel, CLIPTextModel, FlaxCLIPTextModel\nimport jax\nimport jax.numpy as jnp\nfrom tqdm import trange\nimport threading, queue, math\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:48:23.790173Z","iopub.execute_input":"2025-07-16T22:48:23.790345Z","iopub.status.idle":"2025-07-16T22:49:06.279479Z","shell.execute_reply.started":"2025-07-16T22:48:23.790329Z","shell.execute_reply":"2025-07-16T22:49:06.278541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Se torch_xla è installato (TPU), importalo; altrimenti lo ignori\ntry:\n    import torch_xla.core.xla_model as xm\n    TPU_AVAILABLE = True\nexcept ImportError:\n    TPU_AVAILABLE = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:49:06.280965Z","iopub.execute_input":"2025-07-16T22:49:06.281537Z","iopub.status.idle":"2025-07-16T22:49:06.285765Z","shell.execute_reply.started":"2025-07-16T22:49:06.281518Z","shell.execute_reply":"2025-07-16T22:49:06.285109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PromptDatasetManager:\n    def __init__(self, repo_id=\"poloclub/diffusiondb\", filename=\"metadata.parquet\", max_size=None):\n        print(\"Scaricamento metadata.parquet...\")\n        self.meta_path = hf_hub_download(\n            repo_id=repo_id, repo_type=\"dataset\", filename=filename\n        )\n        if max_size:\n            self.df = pd.read_parquet(self.meta_path).head(max_size)\n        else:\n            self.df = pd.read_parquet(self.meta_path)\n        print(f\"Colonne disponibili: {list(self.df.columns)}\\n\")\n        print(f\"Dimensioni del DataFrame: {shape(self.df)}\")\n    \n    def get_path(self):\n        \"\"\"\n        Restituisce il percorso del file metadata.parquet scaricato.\n        \"\"\"\n        return self.meta_path\n\n    def _get_user_sessions(self,\n                          user_name: str,\n                          session_gap: int = 30) -> pd.DataFrame:\n        \"\"\"\n        Estrae dal DataFrame tutte le righe di un dato `user_name`, ordina per timestamp\n        e assegna un `session_id` incrementale ogni volta che l'intervallo tra richieste\n        successive supera `session_gap` minuti.\n        Deve restituire tutte le sessioni per un utente specifico.\n        \"\"\"\n        if 'user_name' not in self.df.columns or 'timestamp' not in self.df.columns:\n            raise ValueError(\"Le colonne 'user_name' e/o 'timestamp' non sono presenti.\")\n        df_user = self.df[self.df['user_name'] == user_name].copy()\n        if df_user.empty:\n            raise ValueError(f\"Nessun dato per l'utente {user_name}\")\n        df_user = df_user.sort_values('timestamp').reset_index(drop=True)\n        # Calcola delta in minuti\n        df_user['session_delta'] = df_user['timestamp'].diff().dt.total_seconds() / 60.0\n        # Assegna session_id\n        session_ids = []\n        current_id = 0\n        for delta in df_user['session_delta']:\n            if pd.isna(delta) or delta > session_gap:\n                current_id += 1\n            session_ids.append(current_id)\n        df_user['session_id'] = session_ids\n        return df_user\n\n\n    def add_clip_embeddings(\n        self,\n        output_path: str,\n        batch_size: int = 512\n    ):\n        \"\"\"\n        Come prima, ma con una progress bar basata sul numero di batch.\n        \"\"\"\n        # 1) Apri il Parquet\n        reader = pq.ParquetFile(self.meta_path)\n        # calcola quante righe e quanti batch\n        total_rows = reader.metadata.num_rows\n        total_batches = math.ceil(total_rows / batch_size)\n\n        # 2) Prepara writer con schema esteso\n        schema_out = reader.schema_arrow.append(\n            pa.field(\"clip_emb\", pa.list_(pa.float32()))\n        )\n        writer = pq.ParquetWriter(output_path, schema_out)\n\n        # 3) Device + modello (MPS + half + compile)\n        device = torch.device(\"mps\") if (torch.backends.mps.is_available() and torch.backends.mps.is_built()) else torch.device(\"cpu\")\n        tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n        model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device).half()\n        model = torch.compile(model)\n        model.eval()\n\n        # 4) Loop con tqdm\n        for batch in tqdm(\n            reader.iter_batches(batch_size=batch_size),\n            desc=\"Calcolo CLIP embeddings\",\n            total=total_batches,\n            unit=\"batch\",\n        ):\n            table = pa.Table.from_batches([batch])\n            prompts = table.column(\"prompt\").to_pylist()\n\n            toks = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\")\n            toks = {k: v.to(device) for k, v in toks.items()}\n\n            with torch.autocast(device_type=device.type, dtype=torch.float16), torch.no_grad():\n                emb = model(**toks).pooler_output.cpu().numpy()\n\n            table = table.append_column(\n                \"clip_emb\",\n                pa.array(emb.tolist(), type=pa.list_(pa.float32()))\n            )\n            writer.write_table(table)\n\n        writer.close()\n        \n        self.df = pd.read_parquet(output_path)\n\n    def add_clip_embeddings_pipelined(self, output_path, batch_size=4096, max_prefetch=2):\n        reader = pq.ParquetFile(self.meta_path)\n        total_rows    = reader.metadata.num_rows\n        total_batches = math.ceil(total_rows / batch_size)\n        schema_out    = reader.schema_arrow.append(pa.field(\"clip_emb\", pa.list_(pa.float32())))\n        writer = pq.ParquetWriter(output_path, schema_out, compression=None)\n\n\n        # setup tokenizer + model\n        device    = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n        tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n        model     = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\\\n                                .to(device).half()\n        model     = torch.compile(model)\n        model.eval()\n\n        q = queue.Queue(max_prefetch)\n\n        # Producer: legge e tokenizza\n        def producer():\n            for batch in reader.iter_batches(batch_size=batch_size):\n                table   = pa.Table.from_batches([batch])\n                prompts = table.column(\"prompt\").to_pylist()\n                toks    = tokenizer(prompts, padding=True, truncation=True, return_tensors=\"pt\")\n                q.put((table, {k: v.to(device) for k, v in toks.items()}))\n            # segnale di fine\n            q.put(None)\n\n        # Consumer: inferisce e scrive\n        def consumer():\n            pbar = tqdm(total=total_batches, desc=\"CLIP embeddings\")\n            while True:\n                item = q.get()\n                if item is None:\n                    break\n                table, toks = item\n                with torch.no_grad():\n                    emb = model(**toks).pooler_output.cpu().numpy()\n                table = table.append_column(\n                    \"clip_emb\",\n                    pa.array(emb.tolist(), type=pa.list_(pa.float32()))\n                )\n                writer.write_table(table)\n\n                pbar.update(1)\n            pbar.close()\n            writer.close()\n\n        t1 = threading.Thread(target=producer, daemon=True)\n        t2 = threading.Thread(target=consumer, daemon=True)\n        t1.start(); t2.start()\n        t1.join(); t2.join()\n        print(f\"✅ Finito, saved to {output_path}\")\n\n\n    def add_clip_embeddings_fast(self, output_path, batch_size=4096, prefetch=2):\n        reader = pq.ParquetFile(self.meta_path)\n        total_batches = math.ceil(reader.metadata.num_rows / batch_size)\n        schema_out = reader.schema_arrow.append(pa.field(\"clip_emb\", pa.list_(pa.float32())))\n        writer = pq.ParquetWriter(output_path, schema_out)\n\n        device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n        tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n        model     = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\\\n                                .to(device).half()\n        model     = torch.compile(model)\n        model.eval()\n\n        q = queue.Queue(prefetch)\n\n        def producer():\n            for batch in reader.iter_batches(batch_size=batch_size):\n                tbl = pa.Table.from_batches([batch])\n                texts = tbl.column(\"prompt\").to_pylist()\n                toks = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n                q.put((tbl, {k:v.to(device) for k,v in toks.items()}))\n            q.put(None)\n\n        def consumer():\n            pbar = tqdm(total=total_batches, desc=\"CLIP embeddings\", unit=\"batch\")\n            while True:\n                item = q.get()\n                if item is None:\n                    break\n                tbl, toks = item\n                with torch.no_grad():\n                    emb = model(**toks).pooler_output.cpu().numpy()\n                tbl = tbl.append_column(\"clip_emb\",\n                    pa.array(emb.tolist(), type=pa.list_(pa.float32())))\n                writer.write_table(tbl)\n                pbar.update()\n            writer.close()\n            pbar.close()\n\n        t1 = threading.Thread(target=producer, daemon=True)\n        t2 = threading.Thread(target=consumer, daemon=True)\n        t1.start(); t2.start()\n        t1.join(); t2.join()\n\n\n\n    def add_clip_embeddings_fast_cuda(\n        self,\n        output_path: str,\n        batch_size: int = 4096,\n        prefetch: int = 2\n    ):\n        \"\"\"\n        Streaming Parquet → batch → tokenizza su CPU → inferisce su GPU (multi‐GPU se disponibile)\n        → scrive il parquet con colonna 'clip_emb' e mostra progress bar.\n        \"\"\"\n        reader = pq.ParquetFile(self.meta_path)\n        total_batches = math.ceil(reader.metadata.num_rows / batch_size)\n\n        # schema di output = schema originale + clip_emb:list<float32>\n        schema_out = reader.schema_arrow.append(\n            pa.field(\"clip_emb\", pa.list_(pa.float32()))\n        )\n        writer = pq.ParquetWriter(output_path, schema_out, compression=None)\n\n        # 1) Device + multi‐GPU\n        if torch.cuda.is_available():\n            device = torch.device(\"cuda\")\n            ngpu = torch.cuda.device_count()\n            print(f\"Using {ngpu} GPU(s): {[torch.cuda.get_device_name(i) for i in range(ngpu)]}\")\n        else:\n            device = torch.device(\"cpu\")\n            ngpu = 0\n            print(\"CUDA non disponibile, uso CPU\")\n\n        # 2) Tokenizer & modello\n        tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n        model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        model = model.to(device).half()  # half precision\n        if ngpu > 1:\n            model = torch.nn.DataParallel(model)\n        model = torch.compile(model)      # PyTorch 2.0+\n        model.eval()\n\n        q = queue.Queue(prefetch)\n\n        # Producer: legge batch e li tokenizza su CPU\n        def producer():\n            for batch in reader.iter_batches(batch_size=batch_size):\n                tbl = pa.Table.from_batches([batch])\n                prompts = tbl.column(\"prompt\").to_pylist()\n                toks = tokenizer(\n                    prompts,\n                    padding=True,\n                    truncation=True,\n                    return_tensors=\"pt\"\n                )\n                q.put((tbl, toks))\n            q.put(None)  # sentinel\n\n        # Consumer: inferisce su GPU e scrive su disco\n        def consumer():\n            pbar = tqdm(total=total_batches, desc=\"CLIP embeddings\", unit=\"batch\")\n            while True:\n                item = q.get()\n                if item is None:\n                    break\n                tbl, toks = item\n                # sposto i tensori sulla GPU\n                toks = {k: v.to(device, non_blocking=True) for k, v in toks.items()}\n                # inferenza in autocast\n                with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")), torch.no_grad():\n                    emb = model(**toks).pooler_output.cpu().numpy()\n                # aggiungo colonna e scrivo\n                tbl = tbl.append_column(\n                    \"clip_emb\",\n                    pa.array(emb.tolist(), type=pa.list_(pa.float32()))\n                )\n                writer.write_table(tbl)\n                pbar.update(1)\n            pbar.close()\n            writer.close()\n\n        t_producer = threading.Thread(target=producer, daemon=True)\n        t_consumer = threading.Thread(target=consumer, daemon=True)\n        t_producer.start()\n        t_consumer.start()\n        t_producer.join()\n        t_consumer.join()\n\n        # ricarica in pandas per usi successivi\n        self.df = pd.read_parquet(output_path)\n        print(f\"✅ Embeddings calcolati e salvati in: {output_path}\")\n\n\n        \n    def add_clip_embeddings_auto(\n        self,\n        output_path: str,\n        batch_size: int = 4096,\n        prefetch: int = 2\n    ):\n        \"\"\"\n        Streaming Parquet → batch → tokenizza su CPU → inferisce su\n        TPU (torch_xla) / GPU (CUDA) / CPU → scrive parquet con colonna `clip_emb`.\n        \"\"\"\n        reader = pq.ParquetFile(self.meta_path)\n        total_rows    = reader.metadata.num_rows\n\n        # --- Device selection e conteggio ---\n        if TPU_AVAILABLE:\n            device    = xm.xla_device()\n            n_devices = xm.xrt_world_size()\n            print(f\"▶ Using TPU: {n_devices} core(s), device={device}\")\n        elif torch.cuda.is_available():\n            n_devices = torch.cuda.device_count()\n            device    = torch.device(\"cuda\")\n            names     = [torch.cuda.get_device_name(i) for i in range(n_devices)]\n            print(f\"▶ Using {n_devices} GPU(s): {names}\")\n        else:\n            n_devices = 1\n            device    = torch.device(\"cpu\")\n            print(\"▶ CUDA/TPU non disponibile, uso CPU\")\n\n        # Adatto batch_size a multiplo di n_devices\n        per_dev_bs     = max(1, batch_size // n_devices)\n        batch_size_glb = per_dev_bs * n_devices\n        if batch_size_glb != batch_size:\n            print(f\"⚙️  Aggiusto batch_size: {batch_size} → {batch_size_glb}  ({per_dev_bs}×{n_devices})\")\n        batch_size    = batch_size_glb\n        total_batches = math.ceil(total_rows / batch_size)\n\n        # --- Writer setup ---\n        schema_out = reader.schema_arrow.append(\n            pa.field(\"clip_emb\", pa.list_(pa.float32()))\n        )\n        writer = pq.ParquetWriter(output_path, schema_out, compression=None)\n\n        # --- Tokenizer & Model setup ---\n        tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n        model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\") \\\n                             .to(device).half()\n        if not TPU_AVAILABLE and n_devices > 1 and device.type == \"cuda\":\n            model = torch.nn.DataParallel(model)\n        model = torch.compile(model)\n        model.eval()\n\n        # Coda per pipeline producer/consumer\n        q = queue.Queue(prefetch)\n\n        def producer():\n            for batch in reader.iter_batches(batch_size=batch_size):\n                tbl     = pa.Table.from_batches([batch])\n                prompts = tbl.column(\"prompt\").to_pylist()\n                toks    = tokenizer(prompts,\n                                    padding=True,\n                                    truncation=True,\n                                    return_tensors=\"pt\")\n                q.put((tbl, toks))\n            q.put(None)\n\n        def consumer():\n            pbar = tqdm(total=total_batches, desc=\"CLIP embeddings\", unit=\"batch\")\n            while True:\n                item = q.get()\n                if item is None:\n                    break\n                tbl, toks = item\n                toks = {k: v.to(device, non_blocking=True) for k, v in toks.items()}\n\n                if TPU_AVAILABLE:\n                    with torch.no_grad():\n                        emb = model(**toks).pooler_output.cpu().numpy()\n                else:\n                    # mixed‑precision solo su CUDA\n                    with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")), torch.no_grad():\n                        emb = model(**toks).pooler_output.cpu().numpy()\n\n                tbl = tbl.append_column(\n                    \"clip_emb\",\n                    pa.array(emb.tolist(), type=pa.list_(pa.float32()))\n                )\n                writer.write_table(tbl)\n                pbar.update(1)\n            pbar.close()\n            writer.close()\n\n        t1 = threading.Thread(target=producer, daemon=True)\n        t2 = threading.Thread(target=consumer, daemon=True)\n        t1.start(); t2.start()\n        t1.join(); t2.join()\n\n        # Ricarico in pandas per usi successivi\n        self.df = pd.read_parquet(output_path)\n        print(f\"✅ Embeddings calcolati e salvati in: {output_path}\")\n      \n                \n\n    \n    \n    def executeFunctionOnDataFrame(self, func, *args, **kwargs):\n        \"\"\"\n        Esegue una funzione `func` sul DataFrame e restituisce il risultato.\n        La funzione deve accettare un DataFrame come primo argomento.\n        \"\"\"\n        if not callable(func):\n            raise ValueError(\"Il parametro 'func' deve essere una funzione chiamabile.\")\n        return func(self.df, *args, **kwargs)\n\n    def  getDataFrame(self):\n        \"\"\"\n        Restituisce il DataFrame completo.\n        Se il DataFrame è molto grande, considera di usare `head()` per limitare le righe.\n        \"\"\"\n        return self.df\n    \n    def getPrompts(self, limit: int = None, shuffle: bool = True):\n        \"\"\"\n        Restituisce una lista di prompt dal DataFrame.\n        Se `limit` è specificato, restituisce solo i primi `limit` prompt.\n        \"\"\"\n        prompts = self.df['prompt'].dropna().tolist()\n        if shuffle:\n            random.shuffle(prompts)\n        if limit is not None:\n            prompts = prompts[:limit]\n        return prompts\n    \n    def getUsersPrompts(self, limit: int = None, shuffle: bool = True):\n        \"\"\"\n        Restituisce un dizionario di prompt per ogni utente.\n        Ogni chiave è il nome dell'utente, e il valore è una lista di prompt.\n        Se `limit` è specificato, restituisce solo i primi `limit` utenti.\n        \"\"\"\n        user_prompts = {}\n        users = self.df['user_name'].dropna().unique()\n        if shuffle:\n            random.shuffle(users)\n        if limit is not None:\n            users = users[:limit]\n        for user in users:\n            user_prompts[user] = self.df[self.df['user_name'] == user]['prompt'].dropna().tolist()\n        return user_prompts\n    \n    def getSessionsPrompts(self, session_gap: int = 30, limit: int = None, shuffle: bool = True):\n        \"\"\"\n        Restituisce un dizionario di sessioni per ogni utente.\n        Ogni chiave è il nome dell'utente, e il valore è un dizionario con `session_id` come chiave\n        e una lista di prompt come valore.\n        Se `limit` è specificato, restituisce solo i primi `limit` utenti.\n        \"\"\"\n        user_sessions = {}\n        users = self.df['user_name'].dropna().unique()\n        if shuffle:\n            random.shuffle(users)\n        if limit is not None:\n            users = users[:limit]\n        for user in users:\n            df_user = self._get_user_sessions(user, session_gap=session_gap)\n            if df_user.empty:\n                continue\n            sessions = defaultdict(list)\n            for _, row in df_user.iterrows():\n                sessions[row['session_id']].append(row['prompt'])\n            user_sessions[user] = dict(sessions)\n        return user_sessions\n    \n    def retRandomPrompt(self):\n        \"\"\"\n        Restituisce una funzione lambda che, ad ogni chiamata,\n        restituisce un nuovo prompt casuale dal database.\n        \"\"\"\n        prompts = self.df['prompt'].dropna().tolist()\n        return lambda: random.choice(prompts)\n    \n    def retRandomSession(self, session_gap: int = 30, max_prompts: int = None):\n        \"\"\"\n        Restituisce una funzione che genera sessioni di un utente casuale.\n        Ogni sessione ritorna i prompt di un utente casuale, se specificato,\n        i prompt per ogni sessione sono limitati a `max_prompts.\n        \"\"\"\n        users = self.df['user_name'].dropna().unique()\n        if len(users) == 0:\n            raise ValueError(\"Nessun utente trovato nel DataFrame.\")\n        \n        def generate_session():\n            user = random.choice(users)\n            df_user = self._get_user_sessions(user, session_gap)\n            if df_user.empty:\n                raise ValueError(f\"Nessuna sessione trovata per l'utente {user}\")\n            # Ritorno tutti i prompt della sessione se `max_prompts` non è specificato\n            if max_prompts is None:\n                return user, df_user['prompt'].dropna().tolist()\n            # Altrimenti, ritorno tutti i prompt della sessione\n            return \n\n        \n        return generate_session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:49:06.286871Z","iopub.execute_input":"2025-07-16T22:49:06.287161Z","iopub.status.idle":"2025-07-16T22:49:06.528941Z","shell.execute_reply.started":"2025-07-16T22:49:06.287137Z","shell.execute_reply":"2025-07-16T22:49:06.528111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"manager = PromptDatasetManager(max_size=10)\nmanager.add_clip_embeddings_auto(\"dataset.parquet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T22:49:06.529884Z","iopub.execute_input":"2025-07-16T22:49:06.530165Z","iopub.status.idle":"2025-07-16T23:05:20.527529Z","shell.execute_reply.started":"2025-07-16T22:49:06.530137Z","shell.execute_reply":"2025-07-16T23:05:20.526764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test\nprint(\"Esempio di utilizzo del PromptDatasetManager:\")\nprint(\"Numero di prompt:\", len(manager.getPrompts()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T23:05:20.52937Z","iopub.execute_input":"2025-07-16T23:05:20.529592Z","iopub.status.idle":"2025-07-16T23:05:21.865418Z","shell.execute_reply.started":"2025-07-16T23:05:20.529574Z","shell.execute_reply":"2025-07-16T23:05:21.864806Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}